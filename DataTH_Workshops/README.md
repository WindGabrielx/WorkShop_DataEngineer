# Workshop_DataTH
Welcome to my workshop! In this session, I will share various case studies and practical examples from my learning journey through DataTH, an online platform offering the "Road to Data Engineer" course. The content here showcases the skills and knowledge Iâ€™ve gained. This workshop covers the following topics:

Workshop1 :
  - Converting data into a Pandas DataFrame
  - Calling an API
  -  Joining two datasets
  - Viewing data in a database using a Pandas DataFrame
  - Saving data after converting from a database

Workshop2 :
  - Set enviroment variable
  - PySpark Setup
  - Data Profiling
  - Data Exploration
  - Data Visualization with Seaborn and Plotly
  - Data Cleansing with Spark
    Timestamp: Incorrect data types and formats

  - Anomalies Check :
    -  Syntactical Anomalies
       Lexical errors such as misspellings
    -  Semantic Anomalies
       Integrity Constraints: Values outside acceptable ranges
    -  Coverage Anomalies
       Missing Values: Checking and replacing empty values
    -  Outliers: Data significantly higher or lower than the majority
    -  Clean Data with Spark SQL
    -  Save Data to CSV

Workshop 3:

  - Comparison between Cloud and On-Premise
  - Introduction to various Cloud concepts, such as Managed Services and Serverless computing
  - Getting to know different services on Google Cloud
  - Comparison of Data Processing and Storage
  - Learning the basics of Command-line (Bash)
    - In this coding session, we will work with both Cloud Shell and Cloud Storage for uploading and downloading files through coding.

Workshop 4:
  - Understanding the tools used for creating Data Pipelines
  - Introduction to the concept of Apache Airflow
  - DAG: Directed Acyclic Graph, Task Dependencies
  - Familiarizing with Cloud Composer
  - Building a data pipeline with Apache Airflow
  - Using the Taskflow API, PythonOperator, BashOperator, DummyOperator
  - Import MySqlHook and edit the Airflow connection to enable connectivity to MySQL


   
